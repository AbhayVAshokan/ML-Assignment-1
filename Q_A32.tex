% roll no 67 Sarithkumar
\textbf{\textcolor{LightMagenta}{ State Occam’s razor principle. Illustrate its necessity in learning hypothesis. (Dec 2018) \hfill 4 marks}} \\[5pt]
{A simple model would generalize better than a complex model. This principle is known as Occam’s razor, which states that simpler explanations are more plausible and any unnecessary complexity should be shaved off.
A common preference relation on the whole hypothesis space is to prefer in the spirit of Occam’s razor simple hypotheses over complicated ones. When choosing a boundary between positive and negative training examples, a hyperplane is e.g. preferred over a non-differentiable surface. Especially, in the presence of noise (i.e. when the labels of the training data may be wrong with some probability) Occam’s razor is often used to avoid the danger of overfitting the training data, that is, to choose a hypothesis that perfectly fits the training data but is very complex and hence often does not generalize well. There has been some discussion on the validity of Occam’s razor (and also of the more or less synonymous overfitting avoidance) also in the machine learning community.
While Occam’s razor often remains a rather vague principle, there are some theoretical results and attempts to clarify what Occam’s razor in machine learning exactly is. Thus, it has been argued [Domingos, 1998] that the term “Occam’s razor” is actually used for two different principles in the machine learning literature. POSTULATE 7 Occam’s first razor. Given two models with the same error on the whole instance space X, choose the simpler one. POSTULATE 8 Occam’s second razor. Given two models with the same error on the training sample, choose the simpler one.}