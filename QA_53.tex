% Roll number 37, Jishnu Pramod
.
\textbf{\textcolor{LightMagenta}{Explain DBSCAN algorithm for density based clustering. List out its
advantages compared to K-means?(Dec 2018) \hfill 10 marks}} \\[5pt]
Clustering analysis is an unsupervised learning method that separates the data points
into several specific bunches or groups, such that the data points in the same groups
have similar properties and data points in different groups have different properties in
some sense.
Density-Based Clustering refers to unsupervised learning methods that identify
distinctive groups/clusters in the data, based on the idea that a cluster in data space is a
contiguous region of high point density, separated from other such clusters by
contiguous regions of low point density\\
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base
algorithm for density-based clustering. It can discover clusters of different shapes and
sizes from a large amount of data, which is containing noise and outliers.\\
The DBSCAN algorithm uses two parameters:



\begin{itemize}
    \item  minPts: The minimum number of points (a threshold) clustered together for a
region to be considered dense.
    \item eps (ε): A distance measure that will be used to locate the points in the
neighborhood of any point.
\end{itemize}

These parameters can be understood if we explore two concepts called Density
Reachability and Density Connectivity.
Reachability in terms of density establishes a point to be reachable from another if it lies
within a particular distance (eps) from it.
Connectivity, on the other hand, involves a transitivity based chaining-approach to
determine whether points are located in a particular cluster. For example, p and q points
could be connected if p->r->s->t->q, where a->b means b is in the neighborhood of a. \\

\textcolor{purple}{Algorithmic steps for DBSCAN clustering}

\begin{itemize}
    \item  The algorithm proceeds by arbitrarily picking up a point in the dataset (until all
points have been visited).
    \item If there are at least ‘minPoint’ points within a radius of ‘ε’ to the point then we
consider all these points to be part of the same cluster
    \item The clusters are then expanded by recursively repeating the neighborhood
calculation for each neighboring point
\end{itemize}

\textcolor{purple}{The Complexity analysis of DBSCAN clustering algorithm}

\begin{itemize}
    \item Best Case: If an indexing system is used to store the dataset such that
neighborhood queries are executed in logarithmic time, we get O(nlogn) average
runtime complexity
    \item Average Case: Same as best/worst case depending on data and implementation
of the algorithm.
\end{itemize}

\textcolor{purple}{Advantages of DBSCAN over K-means clustering}

K-Means clustering may cluster loosely related observations together. Every
observation becomes a part of some cluster eventually, even if the observations are
scattered far away in the vector space. Since clusters depend on the mean value of
cluster elements, each data point plays a role in forming the clusters. A slight change in
data points might affect the clustering outcome. This problem is greatly reduced in
DBSCAN due to the way clusters are formed. This is usually not a big problem unless
we come across some odd shape data.
Another challenge with k-means is that you need to specify the number of clusters (“k”)
in order to use it. Much of the time, we won’t know what a reasonable k value is a priori.
What’s great about DBSCAN is that you don’t have to specify the number of clusters to
use it. All you need is a function to calculate the distance between values and some
guidance for what amount of distance is considered “close”. DBSCAN also produces
more reasonable results than k-means across a variety of different distributions.