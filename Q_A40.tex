% Roll Number 60, Sreeraj Rajan
\textbf{\textcolor{LightMagenta}{What is the significance of optimal separating hyperplanes in SVM?  (May 2019, Q7)\hfill (Mark 4)}} \\[5pt]
In a binary classification problem, given a linearly separable data set, the optimal
separating hyperplane is the one that correctly classifies all the data while being
farthest away from the data points. In this respect, it is said to be the hyperplane
that maximizes the margin, defined as the distance from the hyperplane to the
closest data point.
The optimal separating hyperplane should not be confused with the optimal 
classifier is known as the Bayes classifier: the Bayes classifier is the best classifier 
for a given problem, independently of the available data but unattainable in
practice, whereas the optimal separating hyperplane is only the best linear
classifier one can produce given a particular data set.
The optimal separating hyperplane is one of the core ideas behind the support
vector machines. In particular, it gives rise to the so-called support vectors which are the data points lying on the margin boundary of the hyperplane. These points support the hyperplane in the sense that they contain all the required information to compute the hyperplane: removing other points does not change the optimal separating hyperplane. Elaborating on this fact, one can actually add points to the data set without influencing the hyperplane, as long as these points lie
outside of the margin.
