% Roll number 40, Kiranniya Madhu    
. 
\textbf{\textcolor{LightMagenta}{Explain  any  two  model  combination  scheme  to  improve  the  accuracy  of  a classifier. (Sept 2020) \hfill 4 marks}} \\[5pt]
Ensemble is a machine learning concept in which multiple models are trained using the same learning algorithm.
The principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model. 

Bagging and Boosting are the ensemble learning methods i.e., the methods for combining the predictions from different models.

\textcolor{purple}{\underline{\smash{Bagging}}:} 
The term bagging is also known as bootstrap aggregation. In bagging methods, ensemble model tries to improve prediction accuracy and decrease model variance by combining predictions of individual models trained over randomly generated training samples. The final prediction of ensemble model will be given by calculating the average of all predictions from the individual estimators.

One of the best examples of bagging methods are random forests.

\textcolor{purple}{\underline{\smash{Boosting}}:}
In boosting method, the main principle of building ensemble model is to build it incrementally by training each base model estimator sequentially. As the name suggests, it basically combine several week base learners, trained sequentially over multiple iterations of training data, to build powerful ensemble. During the training of week base learners, higher weights are assigned to those learners which were mis-classified earlier.

An example of boosting method is AdaBoost. 